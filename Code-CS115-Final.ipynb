{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction \n\nThe data includes 297 patient level features including if they have heart disease at the end or not. Features are like:\n\n- Age: Obvious one...\n- Sex:\n    - 0: Female\n    - 1: Male\n- Chest Pain Type: \n    - 0: Typical Angina\n    - 1: Atypical Angina\n    - 2: Non-Anginal Pain\n    - 3: Asymptomatic\n- Resting Blood Pressure: Person's resting blood pressure.\n- Cholesterol: Serum Cholesterol in mg/dl  \n- Fasting Blood Sugar:\n    - 0:Less Than 120mg/ml\n    - 1: Greater Than 120mg/ml\n- Resting Electrocardiographic Measurement:\n    - 0: Normal\n    - 1: ST-T Wave Abnormality\n    - 2: Left Ventricular Hypertrophy\n- Max Heart Rate Achieved: Maximum Heart Rate Achieved\n- Exercise Induced Angina:\n    - 1: Yes\n    - 0: No\n- ST Depression: ST depression induced by exercise relative to rest.\n- Slope: Slope of the peak exercise ST segment:\n    - 0: Upsloping\n    - 1: Flat\n    - 2: Downsloping\n- Thalassemia: A blood disorder called 'Thalassemia':\n    - 0: Normal\n    - 1: Fixed Defect\n    - 2: Reversable Defect\n- Number of Major Vessels: Number of major vessels colored by fluoroscopy."},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading packages.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n#\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nimport math\nimport random\nimport os\nimport time\n\nfrom numpy import interp\n\n# Disabling warnings:\n\nimport warnings\nwarnings.filterwarnings('ignore') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Styling:\n\ncust_palt = [\n    '#111d5e', '#c70039', '#f37121', '#ffbd69', '#ffc93c'\n]\n\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seeding:\n\ndef seed_all(seed):\n    \n    ''' A function to seed everything for getting stable results and reproducibility'''\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed = 42    \nseed_all(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading csv file:\n\ntrain = pd.read_csv('../input/heart-disease-cleveland-uci/heart_cleveland_upload.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking random samples from data.\n\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\n    f'Train data has {train.shape[1]} features, {train.shape[0]} observations.\\nTrain features are:\\n{train.columns.tolist()}\\n'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking null values\n\ntrain.isnull().sum().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rename columns for easier understanding in EDA part."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming columns.\ntrain.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'condition']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique train observartions:\ntrain.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rename the categorical variables for easier EDA interpretation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming cateorical data for easier understanding:\n\ntrain['sex'] = train['sex'].map({0:'female',1:'male'})\ntrain['chest_pain_type'] = train['chest_pain_type'].map({3:'asymptomatic', 1:'atypical_angina', 2:'non_anginal_pain', 0:'typical_angina'})\ntrain['fasting_blood_sugar'] = train['fasting_blood_sugar'].map({0:'less_than_120mg/ml',1:'greater_than_120mg/ml'})\ntrain['rest_ecg'] = train['rest_ecg'].map({0:'normal',1:'ST-T_wave_abnormality',2:'left_ventricular_hypertrophy'})\ntrain['exercise_induced_angina'] = train['exercise_induced_angina'].map({0:'no',1:'yes'})\ntrain['st_slope'] = train['st_slope'].map({0:'upsloping',1:'flat',2:'downsloping'})\ntrain['thalassemia'] = train['thalassemia'].map({1:'fixed_defect',0:'normal',2:'reversable_defect'})\ntrain['condition'] = train['condition'].map({0:'no_disease', 1:'has_disease'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Masks for easier selection in future:\n\ncategorical = [i for i in train.loc[:,train.nunique()<=10]]\ncontinuous = [i for i in train.loc[:,train.nunique()>=10]]\nprint(\"Categorical features: \", categorical)\nprint(\"\\nContinuous features: \", continuous)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"def ctg_dist(df, cols, hue=None,rows=3, columns=3):\n    \n    '''A function for displaying cateorical distribution'''\n    \n    fig, axes = plt.subplots(rows, columns, figsize=(16, 12))\n    axes = axes.flatten()\n\n    for i, j in zip(df[cols].columns, axes):\n        sns.countplot(x=i,\n                    data=df,\n                    palette=cust_palt,\n                    hue=hue,\n                    ax=j,\n                    order=df[i].value_counts().index)\n        j.tick_params(labelrotation=10)\n        \n        total = float(len(df[i]))\n        \n        j.set_title(f'{str(i).capitalize()} Distribution')\n        \n        \n        for p in j.patches:\n            height = p.get_height()\n            j.text(p.get_x() + p.get_width() / 2.,\n                    height + 2,\n                    '{:1.2f}%'.format((height / total) * 100),\n                    ha='center')\n        \n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis\n\n> Univariate analysis is the simplest form of analyzing data. “Uni” means “one”, so in other words your data has only one variable. It doesn't deal with causes or relationships (unlike regression) and it's major purpose is to describe; It takes data, summarizes that data and finds patterns in the data.\n\n#### For this part we going to inspect how's the data distribution is and what patterns we can inspect."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Data\n\n### Here we can do these observations:\n- Males on the dataset is more than double of the female observations.\n- Most common ches pain type is 'Asymptomatic' ones which is almost 50% of the data\n- 85% of the patients has no high levels of fastin blood sugar.\n- Resing electrocardiographic observations are evenly distributed between normal and left ventricular hypertrophy with ST-T minority\n- 67% of the patients had no exercise induced angina\n- Peak exercise slope seems mainly divided between upsloping and flat."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display categorical:\n\nctg_dist(train, categorical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Data\n\n### Most of the continuous variables somewhat close to gaussian distribution with small skews left or right except for oldpeak. Again there are some outliers espacially a strong one in Cholesterol worth to take a look later."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Displaying numeric distribution:\n\nfig = plt.figure(constrained_layout=True, figsize=(16, 12))\n\n\ngrid = gridspec.GridSpec(ncols=6, nrows=3, figure=fig)\n\nax1 = fig.add_subplot(grid[0, :2])\n\nax1.set_title('Trestbps Distribution')\n\nsns.distplot(train[continuous[1]],\n                 hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'alpha': 0.8},\n                 color=cust_palt[0])\n\nax15 = fig.add_subplot(grid[0, 2:3])\n\nax15.set_title('Trestbps')\n\nsns.boxplot(train[continuous[1]], orient='v', color=cust_palt[0])\n\nax2 = fig.add_subplot(grid[0, 3:5])\n\nax2.set_title('Chol Distribution')\n\nsns.distplot(train[continuous[2]],\n                 hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'alpha': 0.8},\n                 color=cust_palt[1])\n\nax25 = fig.add_subplot(grid[0, 5:])\n\nax25.set_title('Chol')\n\nsns.boxplot(train[continuous[2]], orient='v', color=cust_palt[1])\n\nax3 = fig.add_subplot(grid[1, :2])\n\nax3.set_title('Thalach Distribution')\n\nsns.distplot(train[continuous[3]],\n                 hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'alpha': 0.8},\n                 color=cust_palt[2])\n\nax35 = fig.add_subplot(grid[1, 2:3])\n\nax35.set_title('Thalach')\n\nsns.boxplot(train[continuous[3]], orient='v', color=cust_palt[2])\n\nax4 = fig.add_subplot(grid[1, 3:5])\n\nax4.set_title('Oldpeak Distribution')\n\nsns.distplot(train[continuous[4]],\n                 hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'alpha': 0.8},\n                 color=cust_palt[3])\n\nax45 = fig.add_subplot(grid[1, 5:])\n\nax45.set_title('Oldpeak')\n\nsns.boxplot(train[continuous[4]], orient='v', color=cust_palt[3])\n\nax5 = fig.add_subplot(grid[2, :4])\n\nax5.set_title('Age Distribution')\n\nsns.distplot(train[continuous[0]],\n                 hist_kws={\n                 'rwidth': 0.95,\n                 'edgecolor': 'black',\n                 'alpha': 0.8},\n                 color=cust_palt[4])\n\nax55 = fig.add_subplot(grid[2, 4:])\n\nax55.set_title('Age')\n\nsns.boxplot(train[continuous[0]], orient='h', color=cust_palt[4])\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bivariate Analysis\n\n> Bivariate analysis is one of the simplest forms of quantitative analysis. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association.\n\nIn this part we goin to take our variables and compare them against our target condition which is if the observed patient has disease or not."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Data vs Target\n\n### Here we can do these observations:\n\n- Males are much more likely for heart diseases.\n- Chest pain type is very subjective and has no direct relation on the outcome, asymptomatic chest pains having highest disease outcome.\n- Blood sugar has no direct effect on the disease.\n- Rest ECG results showing no direct results but having normal ECG is pretty good sign. Even though it's pretty rare in the data, if you ST-T wave abnormality you are 3 times more likely to have heart disease.\n- Having exercise induced angina is pretty strong indicator for heart disease, patients are almost 3 times more likely to have disease if they have exercise induced angina. Meanwhile it's less than half for not having it.\n- Patients who had flat slope distribution are more likely to have disease.\n- Number of major vessels observed seems on similar levels for patients who have disease but 0 observations is good sign for not having disease.\n- Having defected thalium test results is pretty strong indicator for heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical data vs condition:\n\nctg_dist(train, categorical[:-1], 'condition', 4, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Data vs Target\n\n### Here we can do these observations:\n\n- Having higher resting blood pressure shows you are little bit more likely to have heart disease.\n- Again same for Cholesterol, it's not strong indicator but patients are little bit more likely to have disease with high cholesterol. There's is also one outlier there with no disease, pretty interesting.\n- I find max heart rate distribution a bit interesting, expecting the other way around but it might be due to testing conditions and if you have normal results on ECG while exercising instructors might be increasing your excercise density?\n- It's pretty clear that heart disease likelihood increases with ST depression levels...\n- Lastly older patients are more likely to have heart disease."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Displaying numeric distribution vs condition:\n\nfig = plt.figure(constrained_layout=True, figsize=(16, 12))\n\n\ngrid = gridspec.GridSpec(ncols=4, nrows=3, figure=fig)\n\nax1 = fig.add_subplot(grid[0, :2])\n\nax1.set_title('resting_blood_pressure Distribution')\n\nsns.boxplot(x='condition',\n                    y='resting_blood_pressure',\n                    data=train,\n                    palette=cust_palt[2:],\n                    ax=ax1)\nsns.swarmplot(x='condition',\n                    y='resting_blood_pressure',\n                    data=train,\n                    palette=cust_palt[:2],\n                    ax=ax1)\n\nax2 = fig.add_subplot(grid[0, 2:])\n\nax2.set_title('cholesterol Distribution')\n\nsns.boxplot(x='condition',\n                    y='cholesterol',\n                    data=train,\n                    palette=cust_palt[2:],\n                    ax=ax2)\nsns.swarmplot(x='condition',\n                    y='cholesterol',\n                    data=train,\n                    palette=cust_palt[:2],\n                    ax=ax2)\n\nax3 = fig.add_subplot(grid[1, :2])\n\nax3.set_title('max_heart_rate_achieved Distribution')\n\nsns.boxplot(x='condition',\n                    y='max_heart_rate_achieved',\n                    data=train,\n                    palette=cust_palt[2:],\n                    ax=ax3)\nsns.swarmplot(x='condition',\n                    y='max_heart_rate_achieved',\n                    data=train,\n                    palette=cust_palt[:2],\n                    ax=ax3)\n\nax4 = fig.add_subplot(grid[1, 2:])\n\nax4.set_title('st_depression Distribution')\n\nsns.boxplot(x='condition',\n                    y='st_depression',\n                    data=train,\n                    palette=cust_palt[2:],\n                    ax=ax4)\nsns.swarmplot(x='condition',\n                    y='st_depression',\n                    data=train,\n                    palette=cust_palt[:2],\n                    ax=ax4)\n\nax5 = fig.add_subplot(grid[2, :])\n\nax5.set_title('age Distribution')\n\nsns.boxplot(x='condition',\n                    y='age',\n                    data=train,\n                    palette=cust_palt[2:],\n                    ax=ax5)\nsns.swarmplot(x='condition',\n                    y='age',\n                    data=train,\n                    palette=cust_palt[:2],\n                    ax=ax5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Analysis\n\n> Multivariate analysis (MVA) is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. Typically, MVA is used to address the situations where multiple measurements are made on each experimental unit and the relations among these measurements and their structures are important."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numeric data vs each other and condition:\n\nplt.figure(figsize=(16, 10))\nsns.pairplot(train[['resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression','age', 'condition']], hue='condition', palette=cust_palt,\n           markers=['o','D'], plot_kws=dict(s=25, alpha=0.75, ci=None)\n            )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cholesterol, Max Heart Rate, Age, St Depression vs Target\n\n### Here I tried to fit every single numerical feature into one graph so we can have some visualized version of the effects. 3D scatterplot is great tool for doing that...\n\n#### On X axis we have Cholesterol levels, on Y Max Heart Rate presented and Z axis is patient Age, marker sizes are based on ST_Depression levels and coloring based on the patient condition."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# 3D scatterplot of numeric data:\n\nfig = px.scatter_3d(train, x='cholesterol', y='max_heart_rate_achieved', z='age', size='st_depression',\n              color='condition', opacity=0.8)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspecting Age Closer"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def ctn_freq(df, cols, xaxi, hue=None,rows=4, columns=1):\n    \n    ''' A function for displaying numerical data frequency vs age and condition '''\n    \n    fig, axes = plt.subplots(rows, columns, figsize=(16, 12), sharex=True)\n    axes = axes.flatten()\n\n    for i, j in zip(df[cols].columns, axes):\n        sns.pointplot(x=xaxi,\n                      y=i,\n                    data=df,\n                    palette=cust_palt[:2],\n                    hue=hue,\n                    ax=j,ci=False)      \n        j.set_title(f'{str(i).capitalize()} vs. Age')\n\n        \n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctn_freq(train, ['st_depression','max_heart_rate_achieved','resting_blood_pressure','cholesterol'], 'age', hue='condition',rows=4, columns=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data for corrmap:\n\nheat_train = pd.read_csv('../input/heart-disease-cleveland-uci/heart_cleveland_upload.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlations\n\n#### We going to use pearson correlation for to find linear relations between features, heatmap is decent way to show these relations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation heatmap between variables:\n\nsns.set(font_scale=1.1)\ncorrelation_train = heat_train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 12))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.3f',\n            cmap='Wistia',\n            linewidths=1,\n            cbar=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we chose top related features with outcome condition, seems thal is the most correlated one..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top correlated variables vs condition:\n\ncorrelations = heat_train.corrwith(heat_train['condition']).iloc[:-1].to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\nsorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(sorted_correlations.to_frame()[sorted_correlations>=.35], cmap='Wistia', annot=True, vmin=-1, vmax=1,linewidths=1,fmt='.5f', ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n\n### We start by loading our train data and labels as X and y's and we get dummy variables for categorical data using one hot encoding. Then we import loads of sklearn modules."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting train and condition data:\n\nX = train.drop('condition', axis=1)\ny = heat_train['condition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding train features:\n\nctg_df = pd.get_dummies(data=train[['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg', 'exercise_induced_angina', 'st_slope', 'num_major_vessels', 'thalassemia']])\nX.drop(['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg', 'exercise_induced_angina', 'st_slope', 'num_major_vessels', 'thalassemia'], axis=1, inplace=True)\nX = pd.concat([X, ctg_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ctg_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading sklearn packages:\n\nfrom sklearn.model_selection import cross_validate, KFold, learning_curve,  cross_val_score, RandomizedSearchCV, train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifiers\n\n### Here we selected some common sklearn classifiers.\n\n## DecisionTreeClassifier\n\n> Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n\n## Support Vector Machines\n\n> Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n- The advantages of support vector machines are:\n    - Effective in high dimensional spaces.\n    - Still effective in cases where number of dimensions is greater than the number of samples.\n    - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n    - Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n    \n## GaussianNB\n\n> GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian.\n\n## Logistic Regression\n\n> Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis. \nLogistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n\n\n\n### Ok... Let's get building them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting some sklearn classifiers:\n\ndectree = DecisionTreeClassifier(random_state=seed)\n\nsvc = SVC()\n\ngsclass = GaussianNB()\n\nlgtclass = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting 5 fold CV:\n\ncv = KFold(5, shuffle=True, random_state=seed)\nclassifiers = [dectree, svc, gsclass, lgtclass]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def model_check(X, y, classifiers, cv):\n    \n    ''' A function for testing multiple classifiers and return several metrics. '''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for cls in classifiers:\n\n        MLA_name = cls.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        \n        cv_results = cross_validate(\n            cls,\n            X,\n            y,\n            cv=cv,\n            scoring=('accuracy','f1', 'recall', 'precision'),\n            return_train_score=True,\n            n_jobs=-1\n        )\n        model_table.loc[row_index, 'Train Accuracy Mean'] = cv_results[\n            'train_accuracy'].mean()\n        model_table.loc[row_index, 'Test Accuracy Mean'] = cv_results[\n            'test_accuracy'].mean()\n        model_table.loc[row_index, 'Train Precision Mean'] = cv_results[\n            'train_f1'].mean()\n        model_table.loc[row_index, 'Test Precision Mean'] = cv_results[\n            'test_f1'].mean()\n        model_table.loc[row_index, 'Train Recall Mean'] = cv_results[\n            'train_recall'].mean()\n        model_table.loc[row_index, 'Test Recall Mean'] = cv_results[\n            'test_recall'].mean()\n        model_table.loc[row_index, 'Train F1 Mean'] = cv_results[\n            'train_f1'].mean()\n        model_table.loc[row_index, 'Test F1 Mean'] = cv_results[\n            'test_f1'].mean()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1        \n\n    model_table.sort_values(by=['Test F1 Mean'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Results (sort by f1 score)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline check:\n\nraw_models = model_check(X, y, classifiers, cv)\ndisplay(raw_models)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def f_imp(classifiers, X, y, bins):\n    \n    ''' A function for displaying feature importances'''\n    \n    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n    axes = axes.flatten()\n\n    for ax, classifier in zip(axes, classifiers):\n\n        try:\n            classifier.fit(X, y)\n            feature_imp = pd.DataFrame(sorted(\n                zip(classifier.feature_importances_, X.columns)),\n                                       columns=['Value', 'Feature'])\n\n            sns.barplot(x=\"Value\",\n                        y=\"Feature\",\n                        data=feature_imp.sort_values(by=\"Value\",\n                                                     ascending=False),\n                        ax=ax,\n                        palette='plasma')\n            plt.title('Features')\n            plt.tight_layout()\n            ax.set(title=f'{classifier.__class__.__name__} Feature Impotances')\n            ax.xaxis.set_major_locator(MaxNLocator(nbins=bins))\n        except:\n            continue\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Since our decision tree based models overfitting, we wanted to look which features are most effecting these decisions. We sampled the tree based model you can see below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature importances:\n\nf_imp([dectree], X, y, 6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Automatic Outlier Detection\n\n## Before going to tune our models we decided to get rid of some outliers.\n"},{"metadata":{},"cell_type":"markdown","source":"# Isolation Forest\n\n    The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\n    Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\n    This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\n    Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n\nBasically we set contamination rate of our data to 10% and dropped them using masks. It didn't do great on the results, we have pretty small dataset and removing some more damaging model performances probably. But it's ok for now..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Isolation Forest:\n\niso = IsolationForest(contamination=0.1,random_state=seed)\nyhat = iso.fit_predict(X)\n\nmask = (yhat != -1)\n\nX_iso = X.loc[mask, :]\ny_iso= y[mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking isolated models:\n\niso_models = model_check(X_iso, y_iso, classifiers, cv)\ndisplay(iso_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Elliptic Envelope\n\nLet's try another automatic outlier detection method. We assumed our distribution close to gaussian while inspecting the data so elliptic envelope worth to take a look.\n\n> The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. […] It also serves as a convenient and efficient tool for outlier detection.\n\nThis one did a little bit better than isolation forest so let's stick with it for this case..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Elliptical Envelope:\n\neli = EllipticEnvelope(contamination=0.1,assume_centered=True, random_state=seed)\nyhat = eli.fit_predict(X)\n\nmask = (yhat != -1)\n\nX_eli = X.loc[mask, :]\ny_eli= y[mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli_models = model_check(X_eli, y_eli, classifiers, cv)\ndisplay(eli_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discretization\n\n    Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes.\n\n    One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models.\n\n### Since we have small and noisy data we thougt binning them would be better choice of action.\n\n## K-Bbins Discretization"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def kbin_cat(col, X, nbins=5):\n    \n    ''' A function for binning multiple numeric columns'''\n    \n    categorize = KBinsDiscretizer(n_bins = nbins, encode = 'onehot', strategy = 'kmeans')\n    cat = categorize.fit_transform(X[col].values.reshape(-1,1))\n    cat= pd.DataFrame(cat.toarray())\n    cat_n = [f'cat_{str(i)}' for i in range(nbins)]\n    cat.columns = [i.replace('cat',f'{str(col)}') for i in cat_n]\n    cat = cat.astype('int')\n    \n    return cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying K-bins discretizer:\n\nrt = ['age','resting_blood_pressure','cholesterol', 'max_heart_rate_achieved','st_depression']\nX_cat = X_eli\nfor i in rt:    \n    X_cat = X_cat.join(kbin_cat(i,X,5))\n    X_cat.drop(i, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_cat['age_1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By looking at the results binning improved some models little but few of them like SVC got huge boost to their score. (F1 score 0.44 > 0.80)"},{"metadata":{"trusted":true},"cell_type":"code","source":"binn_models = model_check(X_cat, y_eli, classifiers, cv)\ndisplay(binn_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learning Curves\n\n### Before finalizing our modelling we wanted to use another tool sklearn offers: Learning Curves. That can show us how fast the models learning and especially how is the model doing with the number of data given so we can decide if more data needed for better results. In our case we can see that some models overfitting and most of our models can get better with the more data..."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_learning_curve(classifiers,\n                        X,\n                        y,\n                        ylim=None,\n                        cv=None,\n                        n_jobs=None,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    ''' A function for displaying learning curvers fur multiple ml algorithms'''\n\n    fig, axes = plt.subplots(math.ceil(len(classifiers) / 2),\n                             2,\n                             figsize=(25, 50))\n    axes = axes.flatten()\n\n    for ax, classifier in zip(axes, classifiers):\n\n        ax.set_title(f'{classifier.__class__.__name__} Learning Curve')\n        if ylim is not None:\n            ax.set_ylim(*ylim)\n        ax.set_xlabel('Training examples')\n        ax.set_ylabel('Score')\n\n        train_sizes, train_scores, test_scores, fit_times, _ = \\\n            learning_curve(classifier, X, y, cv=cv, n_jobs=n_jobs,\n                           train_sizes=train_sizes,\n                           return_times=True, scoring='f1', random_state=seed\n                          )\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n\n        # Plot learning curve\n\n        ax.fill_between(train_sizes,\n                        train_scores_mean - train_scores_std,\n                        train_scores_mean + train_scores_std,\n                        alpha=0.1,\n                        color='r')\n        ax.fill_between(train_sizes,\n                        test_scores_mean - test_scores_std,\n                        test_scores_mean + test_scores_std,\n                        alpha=0.1,\n                        color='g')\n        ax.plot(train_sizes,\n                train_scores_mean,\n                'o-',\n                color='r',\n                label='Training score')\n        ax.plot(train_sizes,\n                test_scores_mean,\n                'o-',\n                color='g',\n                label='Cross-validation score')\n        ax.legend(loc='best')\n        ax.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying learning curves:\n\nplot_learning_curve(classifiers,\n                    X_cat,\n                    y_eli,\n                    ylim=None,\n                    cv=cv,\n                    n_jobs=-1,\n                    train_sizes=np.linspace(.1, 1.0, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomizedSearchCV\n\n### Let's get rid of overfitting, one of the easiest ways of doing it is tuning parameters for our estimators and regularize them using RandomizedSearchCV:\n\n    While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n\n    A budget can be chosen independent of the number of parameters and possible values.\n\n    Adding parameters that do not influence the performance does not decrease efficiency.\n\n### We're going to choose small amount of estimators and not many parameters to search for timing purposes."},{"metadata":{},"cell_type":"markdown","source":"### Let's take a look at all the parameters we used in our four models."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVC params used: \", svc.get_params())\nprint(\"\\nDecision Tree params used: \", dectree.get_params())\nprint(\"\\nGaussian Naive Bayes params used: \", gsclass.get_params())\nprint(\"\\nLogistic Regression params used: \", lgtclass.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Searching parameters for fine tuning:\n\nfor i in [svc, dectree, gsclass, lgtclass]:\n    if i == svc:\n        parameters = {\n         'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n         'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000],\n        'kernel': ['linear', 'rbf', 'sigmoid'],\n        'shrinking': [True, False]}\n\n    if i == dectree:\n        parameters = {\n            'max_depth': list(range(1,10)),\n            'criterion':['gini', 'entropy'],\n            'min_samples_leaf': list(range(1,5)),\n             'min_samples_split': list(range(1,10))}\n    \n    if i == gsclass:\n        parameters =  {\n            'var_smoothing': np.logspace(0,-9, num=100)\n        }\n        \n    if i == lgtclass:\n        parameters = {\n            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] \n        }\n    \n    def hyperparameter_tune(base_model, parameters, n_iter, cv, X, y):\n        \n        ''' A function for optimizing mutliple classifiers'''\n        \n        start_time = time.time()\n        optimal_model = RandomizedSearchCV(base_model,\n                                param_distributions=parameters,\n                                n_iter=n_iter,\n                                cv=cv,\n                                scoring = 'f1',\n                                n_jobs=-1,\n                                random_state=seed)\n\n        optimal_model.fit(X, y)    \n\n\n        scores = cross_val_score(optimal_model, X, y, cv=cv,n_jobs=-1, scoring='f1')\n        stop_time = time.time()\n\n        print('====================')\n        print(f'Updated Parameters for {str(base_model.__class__.__name__)}')\n        print('Cross Val Mean: {:.3f}, Cross Val Stdev: {:.3f}'.format(scores.mean(), scores.std()))\n        print('Best Score: {:.3f}'.format(optimal_model.best_score_))\n        print('Best Parameters: {}'.format(optimal_model.best_params_))\n        print('Elapsed Time:', time.strftime('%H:%M:%S', time.gmtime(stop_time - start_time)))\n        print('====================')\n\n\n        return optimal_model.best_params_, optimal_model.best_score_\n    best_params, best_score = hyperparameter_tune(i, parameters, 20, cv, X_cat, y_eli)\n    i.set_params(**best_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuned Model Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking binned models:\n\nbinn_models = model_check(X_cat, y_eli, classifiers, cv)\ndisplay(binn_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimension Reduction Using PCA\n\n    PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components.\n\nReducing dimensions is useful for bigger datasets because by transforming a large set of variables into a smaller one that still contains most of the information in the large set makes your modelling faster. This is not the case here since we have very small data but we still can use it for visualization which I find it cool..."},{"metadata":{},"cell_type":"markdown","source":"### On 2D space we can still diverse the clusters according to our target variables. These two components explains almost one third of the variance..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2 Component PCA:\n\npca = PCA(2)  # project from 46 to 2 dimensions\nmatrix_2d = pca.fit_transform(X_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying 2 PCA:\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig= plt.figure(figsize=(20, 12))\nax =sns.scatterplot(matrix_2d[:, 0], matrix_2d[:, 1],palette=cust_palt[:2],\n            hue=y_eli, alpha=0.9, )\nax.set_title(f'Total Explained Variance: {total_var:.2f}%', fontsize = 20)\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reduced Dimension Model Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_check(matrix_2d, y_eli, classifiers, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Regions\n\n### With these contour plots we can see how the models decide on their predictions based on 2D data with confidence intervals."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prob_reg(X, y):\n    \n    ''' A function for displaying decision regions'''\n    \n    from matplotlib.colors import ListedColormap\n    figure = plt.figure(figsize=(20, 40))\n    h = .02\n    i = 1\n\n    # preprocess dataset, split into training and test part\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.2, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Just plot the dataset first\n    cm = plt.cm.RdYlGn\n    cm_bright = ListedColormap(['#e00d14', '#3ca02c'])\n    ax = plt.subplot(5, 2, i)\n\n    # Iterate over classifiers\n    for clf in classifiers:\n        ax = plt.subplot(math.ceil(len(classifiers) / 2), 2, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, 'decision_function'):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        g = ax.scatter(X_train[:, 0],\n                       X_train[:, 1],\n                       c=y_train,\n                       cmap=cm_bright,\n                       edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0],\n                   X_test[:, 1],\n                   c=y_test,\n                   cmap=cm_bright,\n                   edgecolors='k',\n                   alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n\n        ax.set_title(clf.__class__.__name__)\n\n        ax.set_xlabel('Component 1')\n        ax.set_ylabel('Component 2')\n        plt.legend(handles=g.legend_elements()[0],\n                   labels=['No Disease', 'Has Disease'],\n                   framealpha=0.3,\n                   scatterpoints=1)\n\n        i += 1\n\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_reg(matrix_2d, y_eli)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def conf_mat(X,y, classifiers):\n    \n    ''' A function for displaying confusion matrices'''\n    \n    fig, axes = plt.subplots(4,2, figsize=(20,12))\n    \n    axes = axes.flatten()\n\n    for ax, classifier in zip(axes, classifiers):\n        classifier.fit(X,y)\n        plot_confusion_matrix(classifier, X, y,\n                                         values_format = 'n',\n                                         display_labels = ['No Disease', 'Diease'],\n                                         cmap='summer_r',ax=ax)\n        ax.set_title(f'{classifier.__class__.__name__}')\n        ax.grid(False)\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying confusion matrix for each estimator:\n\nconf_mat(X_cat, y_eli, classifiers)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}